# robots.txt file
#
# Some tips:
# * The lines that start with # are comments
# * URLs are case-senitive
# * The /$ means the match is at the end of the URL path
# * $ at the end means exact path match, which we want to avoid having "Index" match "IndexData"
# * Don't have any extra blank lines because "A blank line indicates a new user agent section", use # for comment space
# * There should be one blank line before each User-agent tag
# * Sitemap: directive
# ** Independent of User-agent
# ** Must be a full URL
# ** Can be more than one
#
# ===========
# User-Agents
# ===========
#
# All user agents are OK

User-agent: *
Crawl-delay: 10
#
# =============================
# Everything else is disallowed
# =============================
Disallow: /

# A particularly aggressive crawler
User-agent: BLEXBot
Disallow: /